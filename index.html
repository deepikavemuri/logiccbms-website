<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LogicCBMs: Logic-Enhanced Concept-Based Learning">
  <meta name="keywords" content="Concepts, Propositional Logic, Differentiable Logic, Interpretability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LogicCBMs: Logic-Enhanced Concept-Based Learning</title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JX0F75QDW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <!-- Spacer to ensure top margin is visible even if CSS is cached/overridden. Remove after CSS confirms behavior. -->
  <div style="height:1.5rem;max-height:48px;">&nbsp;</div>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">LogicCBMs: Logic-Enhanced Concept-Based Learning</h1>
          <h2 class="title is-6 publlication-title">WACV 2026</h2>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://deepikavemuri.github.io/">Deepika SN Vemuri</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Gautham Bellamkonda</a><sup>2</sup><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="">Aditya Pola</a><sup>3</sup><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.iith.ac.in/vineethnb/">Vineeth N Balasubramanian</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>IIT Hyderabad, </span>
            <span class="author-block"><sup>2</sup>Microsoft Research, </span>
            <span class="author-block"><sup>3</sup>KLA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="assets/pdf/2023-LogicCBMs.pdf"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2302.07241"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/deepikavemuri/LogicCBMs"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%"> -->
        <img src="./static/images/logiccbms_teaser.png" alt="LogicCBMs teaser" style="max-width:100%;height:auto;" />
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <span class="coolname">LogicCBMs</span> enhance concept-based learning models by including differentiable logic gates in the network. The model now forms logical 
        compositions of concepts while predicting the class label for a given input image. (Dark shades indicate higher strength in the figure; e.g. Furry is the strongest concept and Cat is the predicted class.)
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="clustering" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="roundabout-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/roundabout nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-baymax" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm baymax.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="text-query-somewhere-to-sit" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/text query somewhere to sit.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="image-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/image query cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="audio-query-doorknock" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/audio query doorknock.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="click-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/click query cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="clustering-outdoors" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering outdoors.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="football-field-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/football field nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-caterpillar" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm caterpillar.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-purple" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm purple.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-spindrift" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm spindrift.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/rkXgws8fiDs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic.
            We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations.
            Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Construct pixel-aligned features</h3>
        <div class="content has-text-justified"> -->
          <p>
            We introduce LogicCBMs, a technique to introduce logic operations into concept-based learning models in an end-to-end differentiable manner.
            As shown in the below figure, we introduce a logic module (which can consist of multiple logic gate layers) following the concept encoder to extract logical predicates, 
            which are then sent to a classifier. There are two steps involved to learn predicates: (i) Concept pairing, and (ii) Differentiable logic learning.
            We introduce two weight matrices: a concept pair matrix \( CP_{p×k} \) and a logic gate matrix \( G_{p×q} \).
          </p>
        <!-- </div> -->
        <br/>
      </div>
    </div>
    <div class="arch-figure">
      <img src="./static/images/logiccbms_arch2.png" alt="LogicCBMs architecture" />
    </div>
    <br/>
    <br/>
    <p>
      In order to determine the concept pairs, we extract the two concepts with the highest weight per row, which gives us a set of \( p \) concept pairs.
      Since logic gates are by themselves non-differentiable operations, they pose a constraint in incorporating them into end-to-end learnable
      architectures. In order to overcome this constraint, we use t-norms, which are the fuzzy versions of logic gates to
      allow differentiability and backpropagation while passing the pairwise activations of the concept encoder through the logic layer.
      For each of the p logic neurons, we compute all \( q \) fuzzy logic operations on the concept pair assigned to it and learn a probability distribution to represent how important a logic
      gate is for that concept pair. The probability distribution for each logic neuron is learned by the G matrix.
    </p>
    <br/>
    <br/>
    <br/>

    <!-- <div class="columns is-centered"> -->

      <!-- Zero-shot pixel alignment -->
      <!-- <div class="column">
        <div class="content">
          <h2 class="title is-3">Zero-shot pixel alignment</h2>
          <div class="interpolation-image-wrapper-zero-shot">
            <img src="./static/images/pixel-aligned-feature-computation.png" />
          </div>
          <p>
            For each image, the global (f<sub>G</sub>) and local (f<sub>L</sub>) features are fused to obtain our pixel-aligned features (f<sub>P</sub>). <i>Top-left</i>: We first compute cosine similarities between each local feature (f<sub>L</sub>) with the global feature (f<sub>G</sub>). <i>Top-right</i>: We compute an inter-feature similarity matrix, and compute the average similarity of each local feature to every other local feature, denoted φ̄<sub>i</sub> . <i>Bottom-left</i>: We combine these similarities to produce weights for fusing f<sub>G</sub> and f<sub>L</sub> to obtain pixel-aligned features f<sub>P</sub>.
          </p>
        </div>
      </div> -->
      <!--/ Zero-shot pixel alignment -->

      <!-- Long-tailed concepts -->
      <!-- <div class="column">
        <h2 class="title is-3">Retaining fine-grained concepts</h2>
        <div class="columns is-centered">
          <div class="column content">
            <div class="interpolation-image-wrapper-fine-grained">
              <img src="./static/images/pixel-aligned-qualitative.png" />
            </div>
            <p>
              Our approach to computing pixel-aligned features is adept at capturing long-tailed and fine-grained concepts. The plots to the right show the similarity scores between the embeddings of the cropped image regions corresponding to diet coke, lysol, and yogurt and their text embeddings, predicted by the base CLIP model used by LSeg and OpenSeg respectively. This implies that the base CLIP models know these concepts, yet, as can be seen on the tiled plots (center), LSeg and OpenSeg are not able to retrieve these concepts; they forget the concepts when finetuned. On the other hand, our zero-shot pixel-alignment approach does not suffer this drawback, and clearly delineates the corresponding pixels.
            </p>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Long-tailed concepts -->


    <div class="columns is-centered">

      <!-- UnCoCo dataset -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">CLEVR-Logic Dataset</h2>
          <p>
            Since real-world datasets do not have explicit logic supervision, it is not straightforward to assess the correctness of the logic. In order to do this, we propose a new variant of CLEVR: <i>CLEVR-Logic</i> which we generated to study logical relations among objects in images.
            In CLEVR-Logic we define concepts to be a set of CLEVR objects (sphere, cone, cube, cylinder) and specify classes as logical operations among these objects. For example, sphere \( \oplus \) cone could be one class which has
            images that exclusively contain either a sphere or a cone. CLEVR images are then generated per class following the class-specific logic. Some examples of some images generated this way are shown below along with their corressponding class-level logic.
          </p>
          <div class="interpolation-image-wrapper-uncoco">
            <img class="clevr-figure" src="./static/images/clevr_logic_dataset.png" alt="CLEVR-Logic dataset visualization" />
          </div>
        </div>
      </div>
      <!--/ UnCoCo dataset -->

      <!-- 3D spatial reasoning -->
      <div class="column">
        <h2 class="title is-3">CCG Metric</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/spatial-query.png" />
            <p>
              <i>What is the distance between the refrigerator from the television?</i>
            </p>
            <p>
              A key benefit of lifting foundation features to 3D is the ability to reason about spatial attributes. We implement a set of generic spatial relationship comparators that can be leveraged for querying arbitrary objects. We employ a large language model to parse the queries to generate function calls that can directly be executed. E.g., the query above parses to <tt>howFar(refrigirator, television)</tt>.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ 3D spatial reasoning -->

    <!-- Long-form text queries -->    
    <!-- <div class="columns is-centered">
      <div class="column is-full-width"> -->

        <!-- Interpolating. -->
        <!-- <h2 class="title is-4">Long-form text queries</h2>
        <div class="content has-text-justified">
          <p>
            LogicCBMs is able to handle long-form text queries and accurately localize objects referenced by the query. In the first two scenarios, OpenSeg is distracted by the presence of several confounding attributes. The third scenario shows a single world query (television) that is part of the COCO Captions dataset used to train OpenSeg, providing it an unfair advantage. LogicCBMs, nonetheless, accurately assigns the highest response to the map points representing the television. In each query, the referenced object is boldfaced.
          </p>
          <img src="./static/images/scannet-text-query.png" />
        </div>
        <br/>
      </div>
    </div> -->
    <!--/ Long-form text queries -->

    <!-- Click-query video -->    
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Click-queries</h2>
        <div class="content has-text-justified">
          <p>
            Click-queries over a sequence from the ICL dataset. For each clicked point, we compute the cosine similarity of the embedding at that point with that of every other map point and visualize them using a 'jet' colormap. Points in red indicate greatest similarity, while points in blue indicate least similarity. Notice the consistency in semantic concepts. For instance, when we click on a point on the corner lamp (at about 0:45), we also notice that the other corner lamp, as well as lights on top of the ceiling get high similarities assigned.
          </p>
          <video id="click-query-icl" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/ICL Click query cropped.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br/>
      </div>
    </div> -->
    <!--/ Click-query video -->

  <!-- </div>
</section> -->


<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments on real robotic systems</h2>
    <div class="columns is-centered"> -->

      <!-- Tabletop rearrangement -->
      <!-- <div class="column">
        <div class="content">
          <h3 class="title is-3">Tabletop rearrangement</h3>
          <div class="interpolation-image-wrapper-tabletop">
            <img src="./static/images/rearrangement.png" />
          </div>
          <p>
            The robot is provided with rearrangment goals involving novel objects. (Top row) push goldfish to the right of the yellow line, where goldfish refers to the brandname of the pack of Cheddar snack. (Bottom row) push baymax to the right of the yellow line, where baymax refers to the plush toy depicting the famous Disney character.
          </p>
        </div>
      </div> -->
      <!--/ Tabletop rearrangement -->

      <!-- Self-driving -->
      <!-- <div class="column">
        <h3 class="title is-3">Autonomous driving</h3>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/driving.png" />
            <p>
              (Left to right; top to bottom) Autonomous drive-by-wire platform deployed; pointcloud map of the environment with the response to the openset text-query ”football field” (shown in red); path found to the football field (shown in red); car successfully navigates to the destination autonomously. See our anonymized webpage for more results.
            </p>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Self-driving -->
  <!-- </div>
</section> -->


<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Integrating LogicCBMs with Large Language Models</h2>
    <div class="columns is-centered"> -->

      <!-- GPT video 1 -->
      <!-- <div class="column">
        <div class="content">
          <video id="gpt-video-1" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ GPT video 1 -->

      <!-- GPT video 2 -->
      <!-- <div class="column">
        <div class="content">
          <video id="gpt-video-2" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ GPT video 2 -->
    <!-- </div>
  </div>
</section> -->


<!-- <script type="text/javascript">
  $(function() {
  var screenWidth = $(window).width();
  if (screenWidth >= 800) {
    $('#gpt-video-1').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#gpt-video-2').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#click-query-icl').attr('autoplay', 'autoplay');
  }
});
</script> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{conceptfusion,
  author    = {Jatavallabhula, {Krishna Murthy} and Kuwajerwala, Alihusein and Gu, Qiao and Omama, Mohd and Chen, Tao and Li, Shuang and Iyer, Ganesh and Saryazdi, Soroush and Keetha, Nikhil and Tewari, Ayush and Tenenbaum, {Joshua B.} and {de Melo}, {Celso Miguel} and Krishna, Madhava and Paull, Liam and Shkurti, Florian and Torralba, Antonio},
  title     = {LogicCBMs: Open-set Multimodal 3D Mapping},
  journal   = {Robotics: Science and Systems (RSS)},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./assets/pdf/2023-LogicCBMs.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/logiccbms" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website adapted from the Nerfies templates, which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/logiccbms/logiccbms.github.io">source code</a> of this website,
            we just ask that you link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a> in the footer.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
